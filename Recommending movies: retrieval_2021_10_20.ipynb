{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“deep_recommenders.ipynb”的副本 2021 10 20",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hai105178362/blogs/blob/main/%E2%80%9Cdeep_recommenders_ipynb%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC_2021_10_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChjuaQjm_iBf"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWqCArLO_kez"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikhIvrku-i-L"
      },
      "source": [
        "# Building deep retrieval models\n",
        "\n",
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/recommenders/examples/deep_recommenders\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/recommenders/blob/main/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/recommenders/blob/main/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/recommenders/docs/examples/deep_recommenders.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrDVNe7Vdqhr"
      },
      "source": [
        "In [the featurization tutorial](featurization) we incorporated multiple features into our models, but the models consist of only an embedding layer. We can add more dense layers to our models to increase their expressive power.\n",
        "\n",
        "In general, deeper models are capable of learning more complex patterns than shallower models. For example, our [user model](featurization#user_model) incorporates user ids and timestamps to model user preferences at a point in time. A shallow model (say, a single embedding layer) may only be able to learn the simplest relationships between those features and movies: a given movie is most popular around the time of its release, and a given user generally prefers horror movies to comedies. To capture more complex relationships, such as user preferences evolving over time, we may need a deeper model with multiple stacked dense layers.\n",
        "\n",
        "Of course, complex models also have their disadvantages. The first is computational cost, as larger models require both more memory and more computation to fit and serve. The second is the requirement for more data: in general, more training data is needed to take advantage of deeper models. With more parameters, deep models might overfit or even simply memorize the training examples instead of learning a function that can generalize. Finally, training deeper models may be harder, and more care needs to be taken in choosing settings like regularization and learning rate.\n",
        "\n",
        "Finding a good architecture for a real-world recommender system is a complex art, requiring good intuition and careful [hyperparameter tuning](https://en.wikipedia.org/wiki/Hyperparameter_optimization). For example, factors such as the depth and width of the model, activation function, learning rate, and optimizer can radically change the performance of the model. Modelling choices are further complicated by the fact that good offline evaluation metrics may not correspond to good online performance, and that the choice of what to optimize for is often more critical than the choice of model itself.\n",
        "\n",
        "Nevertheless, effort put into building and fine-tuning larger models often pays off. In this tutorial, we will illustrate how to build deep retrieval models using TensorFlow Recommenders. We'll do this by building progressively more complex models to see how this affects model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7RYXwgbAcbU"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "We first import the necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgFBaQZEbw3O"
      },
      "source": [
        "!pip install -q tensorflow-recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbwMjnLP5nZ_"
      },
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0LoJ6TBfnld"
      },
      "source": [
        "from typing import Dict, Optional, Text, Tuple, Union\n",
        "import contextlib\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_recommenders import layers\n",
        "\n",
        "def _check_candidates_with_identifiers(candidates: tf.data.Dataset) -> None:\n",
        "  \"\"\"Checks preconditions the dataset used for indexing.\"\"\"\n",
        "\n",
        "  spec = candidates.element_spec\n",
        "\n",
        "  if isinstance(spec, tuple):\n",
        "    if len(spec) != 2:\n",
        "      raise ValueError(\n",
        "          \"The dataset must yield candidate embeddings or \"\n",
        "          \"tuples of (candidate embeddings, candidate identifiers). \"\n",
        "          f\"Got {spec} instead.\"\n",
        "      )\n",
        "\n",
        "    identifiers_spec, candidates_spec = spec\n",
        "\n",
        "    if candidates_spec.shape[0] != identifiers_spec.shape[0]:\n",
        "      raise ValueError(\n",
        "          \"Candidates and identifiers have to have the same batch dimension. \"\n",
        "          f\"Got {candidates_spec.shape[0]} and {identifiers_spec.shape[0]}.\"\n",
        "      )\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def _wrap_batch_too_small_error(k: int):\n",
        "  \"\"\"Context manager that provides a more helpful error message.\"\"\"\n",
        "\n",
        "  try:\n",
        "    yield\n",
        "  except tf.errors.InvalidArgumentError as e:\n",
        "    error_message = str(e)\n",
        "    if \"input must have at least k columns\" in error_message:\n",
        "      raise ValueError(\"Tried to retrieve k={k} top items, but the candidate \"\n",
        "                       \"dataset batch size is too small. This may be because \"\n",
        "                       \"your candidate batch size is too small or the last \"\n",
        "                       \"batch of your dataset is too small. \"\n",
        "                       \"To resolve this, increase your batch size, set the \"\n",
        "                       \"drop_remainder argument to True when batching your \"\n",
        "                       \"candidates, or set the handle_incomplete_batches \"\n",
        "                       \"argument to True in the constructor. \".format(k=k))\n",
        "    else:\n",
        "      raise\n",
        "\n",
        "class Streaming_Lzh(layers.factorized_top_k.TopK):\n",
        "  \"\"\"Retrieves K highest scoring items and their ids from a large dataset.\n",
        "  Used to efficiently retrieve top K query-candidate scores from a dataset,\n",
        "  along with the top scoring candidates' identifiers.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               query_model: Optional[tf.keras.Model] = None,\n",
        "               k: int = 10,\n",
        "               handle_incomplete_batches: bool = True,\n",
        "               num_parallel_calls: int = tf.data.AUTOTUNE,\n",
        "               sorted_order: bool = True) -> None:\n",
        "    \"\"\"Initializes the layer.\n",
        "    Args:\n",
        "      query_model: Optional Keras model for representing queries. If provided,\n",
        "        will be used to transform raw features into query embeddings when\n",
        "        querying the layer. If not provided, the layer will expect to be given\n",
        "        query embeddings as inputs.\n",
        "      k: Number of top scores to retrieve.\n",
        "      handle_incomplete_batches: When True, candidate batches smaller than k\n",
        "        will be correctly handled at the price of some performance. As an\n",
        "        alternative, consider using the drop_remainer option when batching the\n",
        "        candidate dataset.\n",
        "      num_parallel_calls: Degree of parallelism when computing scores. Defaults\n",
        "        to autotuning.\n",
        "      sorted_order: If the resulting scores should be returned in sorted order.\n",
        "        setting this to False may result in a small increase in performance.\n",
        "    Raises:\n",
        "      ValueError if candidate elements are not tuples.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__(k=k)\n",
        "\n",
        "    self.query_model = query_model\n",
        "    self._candidates = None\n",
        "    self._handle_incomplete_batches = handle_incomplete_batches\n",
        "    self._num_parallel_calls = num_parallel_calls\n",
        "    self._sorted = sorted_order\n",
        "\n",
        "    self._counter = self.add_weight(\"counter\", dtype=tf.int32, trainable=False)\n",
        "\n",
        "  def index_from_dataset(\n",
        "      self,\n",
        "      candidates: tf.data.Dataset\n",
        "  ) -> \"TopK\":\n",
        "\n",
        "    _check_candidates_with_identifiers(candidates)\n",
        "\n",
        "    self._candidates = candidates\n",
        "\n",
        "    return self\n",
        "\n",
        "  def index(self,\n",
        "            candidates: tf.data.Dataset,\n",
        "            identifiers: Optional[tf.data.Dataset] = None) -> \"Streaming\":\n",
        "    \"\"\"Not implemented. Please call `index_from_dataset` instead.\"\"\"\n",
        "\n",
        "    raise NotImplementedError(\n",
        "        \"The streaming top k class only accepts datasets. \"\n",
        "        \"Please call `index_from_dataset` instead.\"\n",
        "    )\n",
        "\n",
        "  def call(\n",
        "      self,\n",
        "      queries: Union[tf.Tensor, Dict[Text, tf.Tensor]],\n",
        "      k: Optional[int] = None,\n",
        "  ) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "\n",
        "    k = k if k is not None else self._k\n",
        "\n",
        "    if self._candidates is None:\n",
        "      raise ValueError(\"The `index` method must be called first to \"\n",
        "                       \"create the retrieval index.\")\n",
        "\n",
        "    tf.print(\"_candidates_cardinality:\",tf.data.experimental.cardinality(self._candidates))\n",
        "    for element in self._candidates:\n",
        "      tf.print(\"_candidates_element: \",tf.shape(element))\n",
        "    if self.query_model is not None:\n",
        "      queries = self.query_model(queries)\n",
        "\n",
        "    # Reset the element counter.\n",
        "    self._counter.assign(0)\n",
        "\n",
        "    def top_scores(candidate_index: tf.Tensor,\n",
        "                   candidate_batch: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "      \"\"\"Computes top scores and indices for a batch of candidates.\"\"\"\n",
        "\n",
        "      scores = self._compute_score(queries, candidate_batch)\n",
        "\n",
        "      if self._handle_incomplete_batches:\n",
        "        k_ = tf.math.minimum(k, tf.shape(scores)[1])\n",
        "      else:\n",
        "        k_ = k\n",
        "\n",
        "      scores, indices = tf.math.top_k(scores, k=k_, sorted=self._sorted)\n",
        "\n",
        "      return scores, tf.gather(candidate_index, indices)\n",
        "\n",
        "    def top_k(state: Tuple[tf.Tensor, tf.Tensor],\n",
        "              x: Tuple[tf.Tensor, tf.Tensor]) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "      \"\"\"Reduction function.\n",
        "      Returns top K scores from a combination of existing top K scores and new\n",
        "      candidate scores, as well as their corresponding indices.\n",
        "      Args:\n",
        "        state: tuple of [query_batch_size, k] tensor of highest scores so far\n",
        "          and [query_batch_size, k] tensor of indices of highest scoring\n",
        "          elements.\n",
        "        x: tuple of [query_batch_size, k] tensor of new scores and\n",
        "          [query_batch_size, k] tensor of new indices.\n",
        "      Returns:\n",
        "        Tuple of [query_batch_size, k] tensors of highest scores and indices\n",
        "          from state and x.\n",
        "      \"\"\"\n",
        "      state_scores, state_indices = state\n",
        "      x_scores, x_indices = x\n",
        "\n",
        "      joined_scores = tf.concat([state_scores, x_scores], axis=1)\n",
        "      joined_indices = tf.concat([state_indices, x_indices], axis=1)\n",
        "\n",
        "      if self._handle_incomplete_batches:\n",
        "        k_ = tf.math.minimum(k, tf.shape(joined_scores)[1])\n",
        "      else:\n",
        "        k_ = k\n",
        "\n",
        "      scores, indices = tf.math.top_k(joined_scores, k=k_, sorted=self._sorted)\n",
        "\n",
        "      return scores, tf.gather(joined_indices, indices, batch_dims=1)\n",
        "\n",
        "    def enumerate_rows(batch: tf.Tensor) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "      \"\"\"Enumerates rows in each batch using a total element counter.\"\"\"\n",
        "\n",
        "      starting_counter = self._counter.read_value()\n",
        "      end_counter = self._counter.assign_add(tf.shape(batch)[0])\n",
        "\n",
        "      return tf.range(starting_counter, end_counter), batch\n",
        "\n",
        "    if not isinstance(self._candidates.element_spec, tuple):\n",
        "      # We don't have identifiers.\n",
        "      candidates = self._candidates.map(enumerate_rows)\n",
        "      index_dtype = tf.int32\n",
        "    else:\n",
        "      candidates = self._candidates\n",
        "      index_dtype = self._candidates.element_spec[0].dtype\n",
        "\n",
        "    # Initialize the state with dummy scores and candidate indices.\n",
        "    initial_state = (tf.zeros((tf.shape(queries)[0], 0), dtype=tf.float32),\n",
        "                     tf.zeros((tf.shape(queries)[0], 0), dtype=index_dtype))\n",
        "    \n",
        "    with _wrap_batch_too_small_error(k):\n",
        "      results = (\n",
        "          candidates\n",
        "          # Compute scores over all candidates, and select top k in each batch.\n",
        "          # Each element is a ([query_batch_size, k] tensor,\n",
        "          # [query_batch_size, k] tensor) of scores and indices (where query_\n",
        "          # batch_size is the leading dimension of the input query embeddings).\n",
        "          .map(top_scores, num_parallel_calls=self._num_parallel_calls)\n",
        "          # Reduce into a single tuple of output tensors by keeping a running\n",
        "          # tally of top k scores and indices.\n",
        "          .reduce(initial_state, top_k))\n",
        "\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEvP5B_4O2Gy"
      },
      "source": [
        "from typing import List, Optional, Sequence, Text, Union\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow_recommenders import layers\n",
        "\n",
        "\n",
        "class FactorizedTopK_LZH(tf.keras.layers.Layer):\n",
        "  \"\"\"Computes metrics for across top K candidates surfaced by a retrieval model.\n",
        "  The default metric is top K categorical accuracy: how often the true candidate\n",
        "   is in the top K candidates for a given query.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      candidates: Union[layers.factorized_top_k.TopK, tf.data.Dataset],\n",
        "      metrics: Optional[Sequence[tf.keras.metrics.Metric]] = None,\n",
        "      k: int = 100,\n",
        "      name: Text = \"factorized_top_k\",\n",
        "  ) -> None:\n",
        "    \"\"\"Initializes the metric.\n",
        "    Args:\n",
        "      candidates: A layer for retrieving top candidates in response\n",
        "        to a query, or a dataset of candidate embeddings from which\n",
        "        candidates should be retrieved.\n",
        "      metrics: The metrics to compute. If not supplied, will compute top-K\n",
        "        categorical accuracy metrics.\n",
        "      k: The number of top scoring candidates to retrieve for metric evaluation.\n",
        "      name: Optional name.\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    if metrics is None:\n",
        "      metrics = [\n",
        "          tf.keras.metrics.TopKCategoricalAccuracy(\n",
        "              k=x, name=f\"{self.name}/top_{x}_categorical_accuracy\")\n",
        "          for x in [1, 5, 10, 50, 100]\n",
        "      ]\n",
        "\n",
        "    if isinstance(candidates, tf.data.Dataset):\n",
        "      candidates = Streaming_Lzh(k=k).index_from_dataset(\n",
        "          candidates)\n",
        "      \n",
        "    \n",
        "    self._k = k\n",
        "    self._candidates = candidates\n",
        "    self._top_k_metrics = metrics\n",
        "\n",
        "  def update_state(self, query_embeddings: tf.Tensor,\n",
        "                   true_candidate_embeddings: tf.Tensor) -> tf.Operation:\n",
        "    \"\"\"Updates the metrics.\n",
        "    Args:\n",
        "      query_embeddings: [num_queries, embedding_dim] tensor of query embeddings.\n",
        "      true_candidate_embeddings: [num_queries, embedding_dim] tensor of\n",
        "        embeddings for candidates that were selected for the query.\n",
        "    Returns:\n",
        "      Update op. Only used in graph mode.\n",
        "    \"\"\"\n",
        "    \n",
        "    tf.print(\"query_embeddings.shape\",tf.shape(query_embeddings))\n",
        "    tf.print(\"true_candidate_embeddings.shape\",tf.shape(true_candidate_embeddings))\n",
        "\n",
        "    tf.print(\"query_embeddings\",(query_embeddings),summarize =100)\n",
        "    tf.print(\"true_candidate_embeddings\",(true_candidate_embeddings),summarize =100)\n",
        "    positive_scores = tf.reduce_sum(\n",
        "        query_embeddings * true_candidate_embeddings, axis=1, keepdims=True)\n",
        "    tf.print(\"positive_scores.shape\",tf.shape(positive_scores))\n",
        "    top_k_predictions, _ = self._candidates(query_embeddings, k=self._k)\n",
        "    tf.print(\"top_k_predictions\",tf.shape(top_k_predictions))\n",
        "    tf.print(\"positive_scores\",positive_scores,summarize =100)\n",
        "    #Mixed Negative Sampling\n",
        "    #We propose Mixed Negative Sampling (MNS) to tackle these\n",
        "    #problems. It uniformly samples B′items from another data stream.\n",
        "    #We refer the additional data stream as index data, which is a set\n",
        "    #composed of items from the entire corpus.\n",
        "    y_true = tf.concat(\n",
        "        [tf.ones(tf.shape(positive_scores)),\n",
        "         tf.zeros_like(top_k_predictions)],\n",
        "        axis=1)\n",
        "    y_pred = tf.concat([positive_scores, top_k_predictions], axis=1)\n",
        "    tf.print(\"y_true\",y_true,summarize =100)\n",
        "    tf.print(\"y_pred\",y_pred,summarize =100)\n",
        "    update_ops = []\n",
        "\n",
        "    for metric in self._top_k_metrics:\n",
        "      update_ops.append(metric.update_state(y_true=y_true, y_pred=y_pred))\n",
        "\n",
        "    return tf.group(update_ops)\n",
        "\n",
        "  def reset_states(self) -> None:\n",
        "    \"\"\"Resets the metrics.\"\"\"\n",
        "\n",
        "    for metric in self.metrics:\n",
        "      metric.reset_states()\n",
        "\n",
        "  def result(self) -> List[tf.Tensor]:\n",
        "    \"\"\"Returns a list of metric results.\"\"\"\n",
        "\n",
        "    return [metric.result() for metric in self.metrics]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgKIjpQLAiax"
      },
      "source": [
        "In this tutorial we will use the models from [the featurization tutorial](featurization) to generate embeddings. Hence we will only be using the user id, timestamp, and movie title features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc2REbOO52Fl"
      },
      "source": [
        "import random\n",
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
        "\n",
        "# 点击流数据10000条\n",
        "ratings = ratings.map(lambda x: {    \n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_sex\": random.choice([\"male\",\"female\"]),\n",
        "    \"user_age\": random.choice(range(1, 99)),\n",
        "    \"timestamp\": x[\"timestamp\"],\n",
        "    \"item_id\": x[\"movie_title\"],\n",
        "    \"item_name\": x[\"movie_title\"],\n",
        "    \"item_desc\": x[\"movie_title\"],\n",
        "    \"item_price\": random.choice(range(9999, 13999)),\n",
        "})\n",
        "\n",
        "# 点击流包换 1682个候选集\n",
        "items = movies.map(lambda x: {\n",
        "    \"item_id\": x[\"movie_title\"],\n",
        "    \"item_name\": x[\"movie_title\"],\n",
        "    \"item_desc\": x[\"movie_title\"],\n",
        "    \"item_price\": random.choice(range(9999, 13999)),\n",
        "    })\n",
        "# 更多的候选集，如新发布的产品\n",
        "full_items = movies.map(lambda x: {\n",
        "    \"item_id\": x[\"movie_title\"],\n",
        "    \"item_name\": x[\"movie_title\"],\n",
        "    \"item_desc\": x[\"movie_title\"],\n",
        "    \"item_price\": random.choice(range(9999, 13999)),\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZ2q5RXYNI6"
      },
      "source": [
        "We also do some housekeeping to prepare feature vocabularies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5CVveCS9Doq"
      },
      "source": [
        "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
        "item_prices = np.concatenate(list(items.map(lambda x: x[\"item_price\"]).batch(100)))\n",
        "\n",
        "max_timestamp = timestamps.max()\n",
        "min_timestamp = timestamps.min()\n",
        "\n",
        "timestamp_buckets = np.linspace(\n",
        "    min_timestamp, max_timestamp, num=1000,\n",
        ")\n",
        "\n",
        "\n",
        "unique_item_ids = np.unique(np.concatenate(list(items.batch(1000).map(lambda x: x[\"item_id\"]))))\n",
        "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
        "    lambda x: x[\"user_id\"]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJcCVMUQou3"
      },
      "source": [
        "## Model definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtS6a4sgmI-c"
      },
      "source": [
        "### Query model\n",
        "\n",
        "We start with the user model defined in [the featurization tutorial](featurization) as the first layer of our model, tasked with converting raw input examples into feature embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ItzYwMW42cb"
      },
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.user_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=unique_user_ids, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
        "    ])\n",
        "\n",
        "    self.sex_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=[\"male\",\"female\"], mask_token=None),\n",
        "        tf.keras.layers.Embedding(len([\"male\",\"female\"]) + 1, 32),\n",
        "    ])\n",
        "\n",
        "    #年龄分成几段\n",
        "    self.age_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.Discretization(bin_boundaries=[0., 3., 14., 18., 40., 55., 65.,130.]),\n",
        "        tf.keras.layers.Embedding(len([0., 3., 14., 18., 40., 55., 65.,130.]) + 1, 32),\n",
        "    ])\n",
        "\n",
        "    self.timestamp_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.Discretization(timestamp_buckets.tolist()),\n",
        "        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
        "    ])\n",
        "\n",
        "    self.normalized_timestamp = tf.keras.layers.Normalization(\n",
        "        axis=None\n",
        "    )\n",
        "\n",
        "    self.normalized_timestamp.adapt(timestamps)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Take the input dictionary, pass it through each input layer,\n",
        "    # and concatenate the result.\n",
        "    return tf.concat([\n",
        "        self.user_embedding(inputs[\"user_id\"]),\n",
        "        self.sex_embedding(inputs[\"user_sex\"]),\n",
        "        self.age_embedding(inputs[\"user_age\"]),\n",
        "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
        "        tf.reshape(self.normalized_timestamp(inputs[\"timestamp\"]), (-1, 1)),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMQzxLqh42on"
      },
      "source": [
        "Defining deeper models will require us to stack mode layers on top of this first input. A progressively narrower stack of layers, separated by an activation function, is a common pattern:\n",
        "\n",
        "```\n",
        "                            +----------------------+\n",
        "                            |      128 x 64        |\n",
        "                            +----------------------+\n",
        "                                       | relu\n",
        "                          +--------------------------+\n",
        "                          |        256 x 128         |\n",
        "                          +--------------------------+\n",
        "                                       | relu\n",
        "                        +------------------------------+\n",
        "                        |          ... x 256           |\n",
        "                        +------------------------------+\n",
        "```\n",
        "Since the expressive power of deep linear models is no greater than that of shallow linear models, we use ReLU activations for all but the last hidden layer. The final hidden layer does not use any activation function: using an activation function would limit the output space of the final embeddings and might negatively impact the performance of the model. For instance, if ReLUs are used in the projection layer, all components in the output embedding would be non-negative.\n",
        "\n",
        "We're going to try something similar here. To make experimentation with different depths easy, let's define a model whose depth (and width) is defined by a set of constructor parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qfPi4I-Z0ph"
      },
      "source": [
        "class QueryModel(tf.keras.Model):\n",
        "  \"\"\"Model for encoding user queries.\"\"\"\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    \"\"\"Model for encoding user queries.\n",
        "\n",
        "    Args:\n",
        "      layer_sizes:\n",
        "        A list of integers where the i-th entry represents the number of units\n",
        "        the i-th layer contains.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # We first use the user model for generating embeddings.\n",
        "    self.embedding_model = UserModel()\n",
        "\n",
        "    # Then construct the layers.\n",
        "    self.dense_layers = tf.keras.Sequential()\n",
        "\n",
        "    # Use the ReLU activation for all but the last layer.\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "    # No activation for the last layer.\n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    feature_embedding = self.embedding_model(inputs)\n",
        "    return self.dense_layers(feature_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9IqNTLmpJzs"
      },
      "source": [
        "The `layer_sizes` parameter gives us the depth and width of the model. We can vary it to experiment with shallower or deeper models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XleMceZNHC__"
      },
      "source": [
        "### Candidate model\n",
        "\n",
        "We can adopt the same approach for the movie model. Again, we start with the `MovieModel` from the [featurization](featurization) tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQZHX8bEHPOk"
      },
      "source": [
        "class ItemModel(tf.keras.Model):\n",
        "  \n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    max_tokens = 10_000\n",
        "\n",
        "    self.id_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.StringLookup(\n",
        "          vocabulary=unique_item_ids,mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_item_ids) + 1, 32)\n",
        "    ])\n",
        "\n",
        "    item_name_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
        "    item_name_lookup.adapt(ratings.map(lambda x: x[\"item_name\"]))\n",
        "\n",
        "    self.name_embedding = tf.keras.Sequential([\n",
        "      item_name_lookup, \n",
        "      tf.keras.layers.Embedding(item_name_lookup.vocabulary_size(), 32)])\n",
        "\n",
        "    self.desc_vectorizer = tf.keras.layers.TextVectorization(\n",
        "        max_tokens=max_tokens)\n",
        "\n",
        "    self.desc_text_embedding = tf.keras.Sequential([\n",
        "      self.desc_vectorizer,\n",
        "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "    self.desc_vectorizer.adapt(items.map(lambda x: x[\"item_desc\"]))\n",
        "\n",
        "    self.normalized_price = tf.keras.layers.Normalization(\n",
        "        axis=None\n",
        "    )\n",
        "\n",
        "    self.normalized_price.adapt(item_prices)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.concat([\n",
        "        self.id_embedding(inputs[\"item_id\"]),\n",
        "        self.name_embedding(inputs[\"item_name\"]),\n",
        "        self.desc_text_embedding(inputs[\"item_desc\"]),\n",
        "        tf.reshape(self.normalized_price(inputs[\"item_price\"]), (-1, 1)),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6vssqPYp-gY"
      },
      "source": [
        "And expand it with hidden layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1gTXkvQqHGA"
      },
      "source": [
        "class CandidateModel(tf.keras.Model):\n",
        "  \"\"\"Model for encoding items.\"\"\"\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    \"\"\"Model for encoding items.\n",
        "\n",
        "    Args:\n",
        "      layer_sizes:\n",
        "        A list of integers where the i-th entry represents the number of units\n",
        "        the i-th layer contains.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_model = ItemModel()\n",
        "\n",
        "    # Then construct the layers.\n",
        "    self.dense_layers = tf.keras.Sequential()\n",
        "\n",
        "    # Use the ReLU activation for all but the last layer.\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "    # No activation for the last layer.\n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "    \n",
        "  def call(self, inputs):\n",
        "    feature_embedding = self.embedding_model(inputs)\n",
        "    return self.dense_layers(feature_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc4KbTNwHSvD"
      },
      "source": [
        "### Combined model\n",
        "\n",
        "With both `QueryModel` and `CandidateModel` defined, we can put together a combined model and implement our loss and metrics logic. To make things simple, we'll enforce that the model structure is the same across the query and candidate models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26_hNJPKIh4-"
      },
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    super().__init__()\n",
        "    self.query_model = QueryModel(layer_sizes)\n",
        "    self.candidate_model = CandidateModel(layer_sizes)\n",
        "    self.task = tfrs.tasks.Retrieval(\n",
        "        metrics=FactorizedTopK_LZH(\n",
        "            candidates=items.batch(128).map(lambda x :self.candidate_model(x)),\n",
        "            k = 12,\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    # We only pass the user id and timestamp features into the query model. This\n",
        "    # is to ensure that the training inputs would have the same keys as the\n",
        "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
        "    # error when loading the query model after saving it.\n",
        "    query_embeddings = self.query_model({\n",
        "        \"user_id\": features[\"user_id\"],\n",
        "        \"user_sex\": features[\"user_sex\"],\n",
        "        \"user_age\": features[\"user_age\"],\n",
        "        \"timestamp\": features[\"timestamp\"],\n",
        "    })\n",
        "    item_embeddings = self.candidate_model({\n",
        "        \"item_id\": features[\"item_id\"],\n",
        "        \"item_name\": features[\"item_name\"],\n",
        "        \"item_desc\": features[\"item_desc\"],\n",
        "        \"item_price\": features[\"item_price\"]\n",
        "        })\n",
        "\n",
        "    return self.task(\n",
        "        query_embeddings, item_embeddings, compute_metrics=not training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YXjsRsLTVzt"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY7MTwMruoKh"
      },
      "source": [
        "### Prepare the data\n",
        "\n",
        "We first split the data into a training set and a testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMFUZ4dyTdYd"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80)\n",
        "test = shuffled.skip(80).take(20)\n",
        "\n",
        "cached_train = train.shuffle(100).batch(4)\n",
        "cached_test = test.batch(8).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUOWOdN3gvX5"
      },
      "source": [
        "# No label column specified\n",
        "#dataset_csv = tf.data.experimental.make_csv_dataset(\"/content/sample_data/clickstream_demo.csv\", batch_size=2)\n",
        "#dataset_csv_iterator = dataset_csv.as_numpy_iterator()\n",
        "#print(dict(next(dataset_csv_iterator)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2HEuTBzJ9w5"
      },
      "source": [
        "### Shallow model\n",
        "\n",
        "We're ready to try out our first, shallow, model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkoLkiQdK4Um"
      },
      "source": [
        "num_epochs = 10\n",
        "\n",
        "model = MovielensModel([8])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "one_layer_history = model.fit(\n",
        "    cached_train,\n",
        "    validation_data=cached_test,\n",
        "    validation_freq=5,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0)\n",
        "\n",
        "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
        "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p90vFk8LvJXp"
      },
      "source": [
        "This gives us a top-100 accuracy of around 0.27. We can use this as a reference point for evaluating deeper models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm4VjnSAcSHY"
      },
      "source": [
        "brute_force = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
        "#离线候选集\n",
        "brute_force.index_from_dataset(\n",
        "     items.batch(128).map(lambda x: (x[\"item_id\"], model.candidate_model(x)))\n",
        ")\n",
        "\n",
        "#在线预测\n",
        "_, items_predict = brute_force({\n",
        "        \"user_id\": np.array([\"42\"]),\n",
        "        \"user_sex\": np.array([\"female\"]),\n",
        "        \"user_age\": np.array([29]),\n",
        "        \"timestamp\": np.array([1198090049]),\n",
        "    }, k=3)\n",
        "print(f\"Top recommendations: {items_predict[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_FvTUJRmEbd"
      },
      "source": [
        "brute_force_lots = tfrs.layers.factorized_top_k.BruteForce(model.query_model)\n",
        "brute_force_lots.index_from_dataset(\n",
        "     tf.data.Dataset.zip((full_items.batch(128).map(lambda x: x[\"item_id\"]), full_items.batch(128).map(model.candidate_model)))\n",
        ")\n",
        "\n",
        "#在线预测\n",
        "_, items_predict = brute_force_lots({\n",
        "        \"user_id\": np.array([\"42\"]),\n",
        "        \"user_sex\": np.array([\"female\"]),\n",
        "        \"user_age\": np.array([29]),\n",
        "        \"timestamp\": np.array([1198090049]),\n",
        "    }, k=3)\n",
        "print(f\"Top recommendations: {items_predict[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUvcY5p1oM8p"
      },
      "source": [
        "# Export the query model.\n",
        "with tempfile.TemporaryDirectory() as tmp:\n",
        "  path = os.path.join(\"/content/sample_data\", \"model\")\n",
        "\n",
        "  # Save the index.\n",
        "  tf.saved_model.save(brute_force_lots, path)\n",
        "\n",
        "  # Load it back; can also be done in TensorFlow Serving.\n",
        "  loaded = tf.saved_model.load(path)\n",
        "\n",
        "  # Pass a user id in, get top predicted movie titles back.\n",
        "  scores, titles = loaded({\n",
        "        \"user_id\": np.array([\"42\"]),\n",
        "        \"user_sex\": np.array([\"female\"]),\n",
        "        \"user_age\": np.array([29]),\n",
        "        \"timestamp\": np.array([1198090049]),\n",
        "    })\n",
        "\n",
        "  print(f\"Recommendations: {titles[0][:3]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJ1anzuLXgN"
      },
      "source": [
        "### Deeper model\n",
        "\n",
        "What about a deeper model with two layers?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11qAr5gGMUxE"
      },
      "source": [
        "model = MovielensModel([64, 32])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "two_layer_history = model.fit(\n",
        "    cached_train,\n",
        "    validation_data=cached_test,\n",
        "    validation_freq=5,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0)\n",
        "\n",
        "accuracy = two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
        "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHnzYfQrOj8I"
      },
      "source": [
        "The accuracy here is 0.29, quite a bit better than the shallow model.\n",
        "\n",
        "We can plot the validation accuracy curves to illustrate this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzriiDRlHEvo"
      },
      "source": [
        "num_validation_runs = len(one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"])\n",
        "epochs = [(x + 1)* 5 for x in range(num_validation_runs)]\n",
        "\n",
        "plt.plot(epochs, one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"1 layer\")\n",
        "plt.plot(epochs, two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"2 layers\")\n",
        "plt.title(\"Accuracy vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Top-100 accuracy\");\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ItwGCpXj9YF"
      },
      "source": [
        "Even early on in the training, the larger model has a clear and stable lead over the shallow model, suggesting that adding depth helps the model capture more nuanced relationships in the data.\n",
        "\n",
        "However, even deeper models are not necessarily better. The following model extends the depth to three layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es9k4o0ROt0l"
      },
      "source": [
        "model = MovielensModel([128, 64, 32])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "three_layer_history = model.fit(\n",
        "    cached_train,\n",
        "    validation_data=cached_test,\n",
        "    validation_freq=5,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0)\n",
        "\n",
        "accuracy = three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
        "print(f\"Top-100 accuracy: {accuracy:.2f}.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLJV8jut40Ur"
      },
      "source": [
        "In fact, we don't see improvement over the shallow model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIoVoMO1Kav6"
      },
      "source": [
        "plt.plot(epochs, one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"1 layer\")\n",
        "plt.plot(epochs, two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"2 layers\")\n",
        "plt.plot(epochs, three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"3 layers\")\n",
        "plt.title(\"Accuracy vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Top-100 accuracy\");\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC95C1anA5Gx"
      },
      "source": [
        "This is a good illustration of the fact that deeper and larger models, while capable of superior performance, often require very careful tuning. For example, throughout this tutorial we used a single, fixed learning rate. Alternative choices may give very different results and are worth exploring. \n",
        "\n",
        "With appropriate tuning and sufficient data, the effort put into building larger and deeper models is in many cases well worth it: larger models can lead to substantial improvements in prediction accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB09crfpgBx7"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "In this tutorial we expanded our retrieval model with dense layers and activation functions. To see how to create a model that can perform not only retrieval tasks but also rating tasks, take a look at [the multitask tutorial](multitask)."
      ]
    }
  ]
}
